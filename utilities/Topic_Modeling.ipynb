{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b644f5b",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1) FILE PATHS AND SETTINGS\n",
    "# --------------------------------------------------------------------------\n",
    "CSV_FILE_PATH = \"../data/csv/tweets.csv\" \n",
    "TEXT_COLUMN_NAME = \"Tweet\"        \n",
    "TWEET_EMBEDDINGS_OUTPUT_PATH = \"../data/npy/tweet_embeddings.npy\"\n",
    "TWEETS_METADATA_PATH = \"../data/csv/tweets_metadata.csv\"\n",
    "\n",
    "BATCH_SIZE_EMBEDDING = 256\n",
    "NUM_WORKERS_EMBEDDING = 4\n",
    "TRANSFORMER_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "\n",
    "def embed_tweets():\n",
    "    \"\"\"\n",
    "    Loads all tweets from a CSV file without chunking.\n",
    "    Embeds them via Sentence-BERT and saves:\n",
    "      1) tweet_embeddings.npy\n",
    "      2) tweets_metadata.csv (with tweet text and ID/reference)\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {CSV_FILE_PATH}\")\n",
    "\n",
    "    df_local = pd.read_csv(CSV_FILE_PATH, encoding='utf-8')\n",
    "    if TEXT_COLUMN_NAME not in df_local.columns:\n",
    "        raise ValueError(f\"Column '{TEXT_COLUMN_NAME}' not found in CSV.\")\n",
    "\n",
    "    # Convert all tweets to strings\n",
    "    tweets = df_local[TEXT_COLUMN_NAME].astype(str).tolist()\n",
    "    num_tweets = len(tweets)\n",
    "    print(f\"Loaded {num_tweets} tweets from {CSV_FILE_PATH}.\")\n",
    "\n",
    "    # Optionally keep an ID column if it exists, else create an index-based ID\n",
    "    if \"tweet_id\" not in df_local.columns:\n",
    "        df_local[\"tweet_id\"] = df_local.index + 1  # simple numeric ID\n",
    "\n",
    "    # Save tweet metadata\n",
    "    df_local[[\"tweet_id\", TEXT_COLUMN_NAME]].to_csv(TWEETS_METADATA_PATH, index=False, encoding='utf-8')\n",
    "    print(f\"Saved tweets metadata to {TWEETS_METADATA_PATH}\")\n",
    "\n",
    "    # 2) Embedding\n",
    "    print(f\"Embedding {num_tweets} tweets in batches of {BATCH_SIZE_EMBEDDING}...\")\n",
    "    embed_model = SentenceTransformer(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "    tweet_embeddings_list = []\n",
    "    num_batches = math.ceil(num_tweets / BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE_EMBEDDING\n",
    "        end_idx = min(start_idx + BATCH_SIZE_EMBEDDING, num_tweets)\n",
    "        batch_texts = tweets[start_idx:end_idx]\n",
    "\n",
    "        # Encode this batch\n",
    "        batch_embeds = embed_model.encode(\n",
    "            batch_texts,\n",
    "            show_progress_bar=False,\n",
    "            batch_size=8,  # smaller sub-batch to reduce memory usage\n",
    "            num_workers=NUM_WORKERS_EMBEDDING\n",
    "        )\n",
    "\n",
    "        tweet_embeddings_list.extend(batch_embeds)\n",
    "        print(f\"Processed batch {batch_idx+1}/{num_batches} ({len(batch_texts)} tweets).\")\n",
    "\n",
    "        del batch_texts, batch_embeds\n",
    "        gc.collect()\n",
    "\n",
    "    # Convert embeddings to float32 to save memory\n",
    "    tweet_embeddings = np.array(tweet_embeddings_list, dtype=np.float32)\n",
    "    print(f\"Final embeddings shape: {tweet_embeddings.shape}\")\n",
    "\n",
    "    # Save embeddings to disk\n",
    "    np.save(TWEET_EMBEDDINGS_OUTPUT_PATH, tweet_embeddings)\n",
    "    print(f\"Embeddings saved to {TWEET_EMBEDDINGS_OUTPUT_PATH}\")\n",
    "\n",
    "    del tweet_embeddings_list, tweet_embeddings, embed_model\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embed_tweets()\n",
    "    print(\"Step 1 (tweets embedding) completed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import umap\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) FILE PATHS\n",
    "# -----------------------------------------------------------------------------\n",
    "TWEET_EMBEDDINGS_OUTPUT_PATH = \"data/npy/tweet_embeddings.npy\"\n",
    "TWEETS_METADATA_PATH         = \"data/csv/tweets_metadata.csv\"\n",
    "\n",
    "TWEETS_WITH_TOPICS_CSV       = \"data/csv/tweets_with_topics.csv\"\n",
    "TOPIC_INFO_CSV               = \"data/csv/tweets_topic_info.csv\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) HYPERPARAMETERS FOR TOPIC MODELING\n",
    "# -----------------------------------------------------------------------------\n",
    "STOP_WORDS      = set(stopwords.words('english'))\n",
    "STEMMER         = PorterStemmer()\n",
    "\n",
    "NO_BELOW        = 2      # Filter out tokens that appear in fewer than 2 tweets\n",
    "NO_ABOVE        = 0.90   # Filter out tokens that appear in more than 90% of tweets\n",
    "\n",
    "UMAP_N_NEIGHBORS   = 15\n",
    "UMAP_N_COMPONENTS  = 10\n",
    "MIN_TOPIC_SIZE     = 5\n",
    "N_GRAM_RANGE       = (1, 3)\n",
    "NR_TOPICS          = 25\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) BASIC PREPROCESSING\n",
    "# -----------------------------------------------------------------------------\n",
    "def preprocess_text(text: str):\n",
    "    \"\"\"\n",
    "    Lowercase, remove non-alpha, remove stopwords & short tokens, and apply stemming.\n",
    "    Adjust if you have different preprocessing preferences.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Remove anything not alphabetical or space\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    # Filter out short tokens and stopwords\n",
    "    tokens = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\n",
    "    # Stem each token\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) OPTIONAL: OUTLIER REASSIGNMENT\n",
    "# -----------------------------------------------------------------------------\n",
    "def reassign_outliers_to_nearest_cluster(assigned_topics, embeddings):\n",
    "    \"\"\"\n",
    "    BERTopic sometimes assigns -1 to outlier documents.\n",
    "    This function reassigns outlier tweets to the closest non-outlier cluster\n",
    "    via nearest neighbors (cosine or Euclidean distance).\n",
    "    \"\"\"\n",
    "    assigned_topics = np.array(assigned_topics)\n",
    "    outlier_mask = (assigned_topics == -1)\n",
    "    if not outlier_mask.any():\n",
    "        return assigned_topics  # No outliers => no changes\n",
    "\n",
    "    print(f\"Found {outlier_mask.sum()} outliers. Reassigning them to the nearest cluster...\")\n",
    "\n",
    "    # Split embeddings into outliers vs. non-outliers\n",
    "    non_outlier_mask = ~outlier_mask\n",
    "    non_outlier_embeddings = embeddings[non_outlier_mask]\n",
    "    non_outlier_topics = assigned_topics[non_outlier_mask]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(non_outlier_embeddings)\n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    outlier_embeddings = embeddings[outlier_indices]\n",
    "\n",
    "    distances, indices = nbrs.kneighbors(outlier_embeddings)\n",
    "    for i, idx in enumerate(indices[:, 0]):\n",
    "        outlier_idx = outlier_indices[i]\n",
    "        assigned_topics[outlier_idx] = non_outlier_topics[idx]\n",
    "\n",
    "    return assigned_topics\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) MAIN PIPELINE: LOAD EMBEDDINGS, PREPROCESS TEXT, FIT BERTopic\n",
    "# -----------------------------------------------------------------------------\n",
    "def fit_topic_model():\n",
    "    # 1) Check file existence\n",
    "    if not os.path.exists(TWEETS_METADATA_PATH):\n",
    "        raise FileNotFoundError(f\"Tweet metadata not found at: {TWEETS_METADATA_PATH}\")\n",
    "    if not os.path.exists(TWEET_EMBEDDINGS_OUTPUT_PATH):\n",
    "        raise FileNotFoundError(f\"Tweet embeddings not found at: {TWEET_EMBEDDINGS_OUTPUT_PATH}\")\n",
    "\n",
    "    # 2) Load metadata and embeddings\n",
    "    df_tweets = pd.read_csv(TWEETS_METADATA_PATH, encoding='utf-8')\n",
    "    tweet_embeddings = np.load(TWEET_EMBEDDINGS_OUTPUT_PATH)\n",
    "\n",
    "    print(f\"Number of tweets loaded: {len(df_tweets)}\")\n",
    "    print(f\"Embeddings shape: {tweet_embeddings.shape}\")\n",
    "\n",
    "    # 3) Preprocess text & filter out empty tokens\n",
    "    df_tweets[\"tokens\"] = df_tweets[\"Tweet\"].astype(str).apply(preprocess_text)\n",
    "    df_tweets = df_tweets[df_tweets[\"tokens\"].apply(lambda x: len(x) > 0)]\n",
    "    print(f\"Number of tweets after removing empty tokens: {len(df_tweets)}\")\n",
    "\n",
    "    # 4) Filter extreme tokens via Gensim Dictionary\n",
    "    dictionary = Dictionary(df_tweets[\"tokens\"])\n",
    "    dictionary.filter_extremes(no_below=NO_BELOW, no_above=NO_ABOVE)\n",
    "\n",
    "    # Rebuild tokens so only in-dictionary words remain\n",
    "    filtered_tokens = []\n",
    "    for tok_list in df_tweets[\"tokens\"]:\n",
    "        filtered = [w for w in tok_list if w in dictionary.token2id]\n",
    "        filtered_tokens.append(filtered)\n",
    "\n",
    "    df_tweets[\"tokens\"] = filtered_tokens\n",
    "    df_tweets = df_tweets[df_tweets[\"tokens\"].apply(lambda x: len(x) > 0)]\n",
    "    print(f\"Number of tweets after dictionary filtering: {len(df_tweets)}\")\n",
    "\n",
    "    # 5) Filter embeddings to match the final set of tweets\n",
    "    #    Because some tweets may have been dropped if they had no valid tokens.\n",
    "    retained_indices = df_tweets.index.values  # the current DataFrame index\n",
    "    filtered_embeddings = tweet_embeddings[retained_indices, :]\n",
    "    print(f\"Filtered embeddings shape: {filtered_embeddings.shape}\")\n",
    "\n",
    "    # 6) Prepare text input for BERTopic\n",
    "    text_for_model = [\" \".join(tok_list) for tok_list in df_tweets[\"tokens\"]]\n",
    "\n",
    "    # 7) Configure BERTopic\n",
    "    my_umap = umap.UMAP(\n",
    "        n_neighbors=UMAP_N_NEIGHBORS,\n",
    "        n_components=UMAP_N_COMPONENTS,\n",
    "        min_dist=0.1\n",
    "    )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        umap_model=my_umap,\n",
    "        min_topic_size=MIN_TOPIC_SIZE,\n",
    "        n_gram_range=N_GRAM_RANGE,\n",
    "        nr_topics=NR_TOPICS,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 8) Fit BERTopic\n",
    "    print(\"Fitting BERTopic on tweets...\")\n",
    "    assigned_topics, _ = topic_model.fit_transform(text_for_model, filtered_embeddings)\n",
    "\n",
    "    # 9) Optionally reassign outliers from -1\n",
    "    assigned_topics = reassign_outliers_to_nearest_cluster(assigned_topics, filtered_embeddings)\n",
    "    df_tweets[\"topic_id\"] = assigned_topics\n",
    "\n",
    "    # 10) Extract topic info\n",
    "    topic_info_df = topic_model.get_topic_info()\n",
    "    print(\"BERTopic modeling complete.\")\n",
    "    print(\"=== Sample of topic info ===\")\n",
    "    print(topic_info_df.head(10))\n",
    "\n",
    "    # 11) Save results\n",
    "    df_tweets.to_csv(TWEETS_WITH_TOPICS_CSV, index=False, encoding='utf-8')\n",
    "    topic_info_df.to_csv(TOPIC_INFO_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Saved tweets with topic assignments to: {TWEETS_WITH_TOPICS_CSV}\")\n",
    "    print(f\"Saved topic info to: {TOPIC_INFO_CSV}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del tweet_embeddings, filtered_embeddings\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fit_topic_model()\n",
    "    print(\"Topic modeling step completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
